{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T07:07:09.118538Z",
     "start_time": "2019-11-10T07:07:01.295472Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import math\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import CuDNNLSTM,Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data and creating a standard linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T07:07:09.472562Z",
     "start_time": "2019-11-10T07:07:09.119508Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-53d00bbcf883>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "input_shape = mnist.train.images.shape[1]\n",
    "num_classes = mnist.train.labels.shape[1]\n",
    "batch_size = 128\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T07:07:09.547362Z",
     "start_time": "2019-11-10T07:07:09.473560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1024, activation='relu',input_shape=(input_shape,), kernel_initializer='glorot_normal'))\n",
    "model.add(Dense(1024, activation='relu', kernel_initializer='glorot_normal'))\n",
    "model.add(Dense(1024, activation='relu', kernel_initializer='glorot_normal'))\n",
    "model.add(Dense(1024, activation='relu', kernel_initializer='glorot_normal'))\n",
    "model.add(Dense(1024, activation='relu', kernel_initializer='glorot_normal'))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T07:07:09.552349Z",
     "start_time": "2019-11-10T07:07:09.548362Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1024)              803840    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 5,012,490\n",
      "Trainable params: 5,012,490\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T07:08:13.092099Z",
     "start_time": "2019-11-10T07:07:09.553346Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\keras\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "55000/55000 [==============================] - 4s 81us/step - loss: 0.2438 - acc: 0.9270 - val_loss: 0.1200 - val_acc: 0.9632\n",
      "Epoch 2/20\n",
      "55000/55000 [==============================] - 3s 52us/step - loss: 0.1003 - acc: 0.9712 - val_loss: 0.1032 - val_acc: 0.9702\n",
      "Epoch 3/20\n",
      "55000/55000 [==============================] - 3s 54us/step - loss: 0.0737 - acc: 0.9785 - val_loss: 0.0800 - val_acc: 0.9780\n",
      "Epoch 4/20\n",
      "55000/55000 [==============================] - 3s 53us/step - loss: 0.0603 - acc: 0.9831 - val_loss: 0.1155 - val_acc: 0.9728\n",
      "Epoch 5/20\n",
      "55000/55000 [==============================] - 3s 52us/step - loss: 0.0449 - acc: 0.9870 - val_loss: 0.1202 - val_acc: 0.9756\n",
      "Epoch 6/20\n",
      "55000/55000 [==============================] - 3s 52us/step - loss: 0.0505 - acc: 0.9859 - val_loss: 0.1030 - val_acc: 0.9784\n",
      "Epoch 7/20\n",
      "55000/55000 [==============================] - 3s 53us/step - loss: 0.0373 - acc: 0.9896 - val_loss: 0.1086 - val_acc: 0.9784\n",
      "Epoch 8/20\n",
      "55000/55000 [==============================] - 3s 56us/step - loss: 0.0320 - acc: 0.9915 - val_loss: 0.0938 - val_acc: 0.9804\n",
      "Epoch 9/20\n",
      "55000/55000 [==============================] - 3s 52us/step - loss: 0.0315 - acc: 0.9912 - val_loss: 0.1113 - val_acc: 0.9774\n",
      "Epoch 10/20\n",
      "55000/55000 [==============================] - 3s 53us/step - loss: 0.0303 - acc: 0.9923 - val_loss: 0.0887 - val_acc: 0.9796\n",
      "Epoch 11/20\n",
      "55000/55000 [==============================] - 3s 57us/step - loss: 0.0240 - acc: 0.9937 - val_loss: 0.0972 - val_acc: 0.9810\n",
      "Epoch 12/20\n",
      "55000/55000 [==============================] - 3s 56us/step - loss: 0.0247 - acc: 0.9940 - val_loss: 0.1054 - val_acc: 0.9802\n",
      "Epoch 13/20\n",
      "55000/55000 [==============================] - 3s 58us/step - loss: 0.0211 - acc: 0.9949 - val_loss: 0.1147 - val_acc: 0.9792\n",
      "Epoch 14/20\n",
      "55000/55000 [==============================] - 3s 58us/step - loss: 0.0248 - acc: 0.9937 - val_loss: 0.0984 - val_acc: 0.9810\n",
      "Epoch 15/20\n",
      "55000/55000 [==============================] - 4s 64us/step - loss: 0.0201 - acc: 0.9952 - val_loss: 0.1187 - val_acc: 0.9806\n",
      "Epoch 16/20\n",
      "55000/55000 [==============================] - 3s 59us/step - loss: 0.0266 - acc: 0.9936 - val_loss: 0.1171 - val_acc: 0.9808\n",
      "Epoch 17/20\n",
      "55000/55000 [==============================] - 3s 63us/step - loss: 0.0214 - acc: 0.9948 - val_loss: 0.0951 - val_acc: 0.9844\n",
      "Epoch 18/20\n",
      "55000/55000 [==============================] - 3s 58us/step - loss: 0.0191 - acc: 0.9949 - val_loss: 0.0947 - val_acc: 0.9836\n",
      "Epoch 19/20\n",
      "55000/55000 [==============================] - 3s 57us/step - loss: 0.0134 - acc: 0.9967 - val_loss: 0.1138 - val_acc: 0.9814\n",
      "Epoch 20/20\n",
      "55000/55000 [==============================] - 3s 57us/step - loss: 0.0186 - acc: 0.9956 - val_loss: 0.1120 - val_acc: 0.9806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e944ede048>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(mnist.train.images, mnist.train.labels,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(mnist.validation.images, mnist.validation.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T07:08:13.571807Z",
     "start_time": "2019-11-10T07:08:13.093088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 47us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08901699159120663, 0.9847]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(mnist.test.images, mnist.test.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the weights and bias of all six layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T07:08:13.644612Z",
     "start_time": "2019-11-10T07:08:13.572811Z"
    }
   },
   "outputs": [],
   "source": [
    "w1 =  model.layers[0].get_weights()[0]\n",
    "w2 =  model.layers[1].get_weights()[0]\n",
    "w3 =  model.layers[2].get_weights()[0]\n",
    "w4 =  model.layers[3].get_weights()[0]\n",
    "w5 =  model.layers[4].get_weights()[0]\n",
    "w6 =  model.layers[5].get_weights()[0]\n",
    "\n",
    "b1 =  model.layers[0].get_weights()[1]\n",
    "b2 =  model.layers[1].get_weights()[1]\n",
    "b3 =  model.layers[2].get_weights()[1]\n",
    "b4 =  model.layers[3].get_weights()[1]\n",
    "b5 =  model.layers[4].get_weights()[1]\n",
    "b6 =  model.layers[5].get_weights()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting U, S, & V via using SVD from numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T07:08:15.214410Z",
     "start_time": "2019-11-10T07:08:13.645609Z"
    }
   },
   "outputs": [],
   "source": [
    "u1, s1, v1 = np.linalg.svd(w1, full_matrices=False)\n",
    "u2, s2, v2 = np.linalg.svd(w2, full_matrices=False)\n",
    "u3, s3, v3 = np.linalg.svd(w3, full_matrices=False)\n",
    "u4, s4, v4 = np.linalg.svd(w4, full_matrices=False)\n",
    "u5, s5, v5 = np.linalg.svd(w5, full_matrices=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing different values of D for our approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T07:08:18.403874Z",
     "start_time": "2019-11-10T07:08:15.215408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 49us/step\n",
      "10000/10000 [==============================] - 0s 48us/step\n",
      "10000/10000 [==============================] - 0s 47us/step\n",
      "10000/10000 [==============================] - 0s 45us/step\n",
      "10000/10000 [==============================] - 0s 45us/step\n",
      "10000/10000 [==============================] - 0s 43us/step\n",
      "[0.692, 0.9436, 0.9813, 0.9841, 0.9845, 0.9848]\n"
     ]
    }
   ],
   "source": [
    "dnum = [10, 20, 50, 100, 200, input_shape]\n",
    "results = []\n",
    "for x in dnum:\n",
    "    tw1 = np.dot(u1[:,:x], np.dot(np.diag(s1)[:x,:x], v1[:x,:]))\n",
    "    tw2 = np.dot(u2[:,:x], np.dot(np.diag(s2)[:x,:x], v2[:x,:]))\n",
    "    tw3 = np.dot(u3[:,:x], np.dot(np.diag(s3)[:x,:x], v3[:x,:]))\n",
    "    tw4 = np.dot(u4[:,:x], np.dot(np.diag(s4)[:x,:x], v4[:x,:]))\n",
    "    tw5 = np.dot(u5[:,:x], np.dot(np.diag(s4)[:x,:x], v5[:x,:]))\n",
    "\n",
    "    model.layers[0].set_weights((tw1,b1))\n",
    "    model.layers[1].set_weights((tw2,b2))\n",
    "    model.layers[2].set_weights((tw3,b3))\n",
    "    model.layers[3].set_weights((tw4,b5))\n",
    "    model.layers[4].set_weights((tw5,b5))\n",
    "\n",
    "    results.append(model.evaluate(mnist.test.images, mnist.test.labels)[1])\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T07:08:18.517570Z",
     "start_time": "2019-11-10T07:08:18.404872Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(dnum, results, '.b-')\n",
    "plt.xlabel(\"D\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using D  = 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T07:08:18.541506Z",
     "start_time": "2019-11-10T07:08:18.518568Z"
    }
   },
   "outputs": [],
   "source": [
    "x  = 20\n",
    "\n",
    "nu1 = u1[:,:x]\n",
    "nu2 = u2[:,:x]\n",
    "nu3 = u3[:,:x]\n",
    "nu4 = u4[:,:x]\n",
    "nu5 = u5[:,:x]\n",
    "\n",
    "nv1 = np.dot(np.diag(s1)[:x,:x], v1[:x,:])\n",
    "nv2 = np.dot(np.diag(s2)[:x,:x], v2[:x,:])\n",
    "nv3 = np.dot(np.diag(s3)[:x,:x], v3[:x,:])\n",
    "nv4 = np.dot(np.diag(s4)[:x,:x], v4[:x,:])\n",
    "nv5 = np.dot(np.diag(s4)[:x,:x], v5[:x,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting 5 hidden layer model to 10 hidden layer model with out svd approximations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T07:08:19.520886Z",
     "start_time": "2019-11-10T07:08:18.542503Z"
    }
   },
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "\n",
    "#layer 1\n",
    "model3.add(Dense(20, activation='relu',input_shape=(input_shape,), weights=(nu1,np.zeros(20))))\n",
    "model3.add(Dense(1024, activation='relu', weights=(nv1,np.zeros(1024))))\n",
    "\n",
    "#layer 2\n",
    "model3.add(Dense(20, activation='relu', weights=(nu2,np.zeros(20))))\n",
    "model3.add(Dense(1024, activation='relu', weights=(nv2,np.zeros(1024))))\n",
    "\n",
    "#layer 3\n",
    "model3.add(Dense(20, activation='relu', weights=(nu3,np.zeros(20))))\n",
    "model3.add(Dense(1024, activation='relu', weights=(nv3,np.zeros(1024))))\n",
    "\n",
    "#layer 4\n",
    "model3.add(Dense(20, activation='relu', weights=(nu4,np.zeros(20))))\n",
    "model3.add(Dense(1024, activation='relu', weights=(nv4,np.zeros(1024))))\n",
    "\n",
    "#layer 5\n",
    "model3.add(Dense(20, activation='relu', weights=(nu5,np.zeros(20))))\n",
    "model3.add(Dense(1024, activation='relu', weights=(nv5,np.zeros(1024))))\n",
    "\n",
    "#layer 6\n",
    "model3.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T07:08:19.525876Z",
     "start_time": "2019-11-10T07:08:19.521883Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 20)                15700     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1024)              21504     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 20)                20500     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1024)              21504     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 20)                20500     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1024)              21504     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 20)                20500     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1024)              21504     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 20)                20500     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1024)              21504     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 215,470\n",
      "Trainable params: 215,470\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T07:09:15.095661Z",
     "start_time": "2019-11-10T07:08:19.527867Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "55000/55000 [==============================] - 3s 56us/step - loss: 0.2977 - acc: 0.9107 - val_loss: 0.1551 - val_acc: 0.9614\n",
      "Epoch 2/20\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.1268 - acc: 0.9660 - val_loss: 0.1396 - val_acc: 0.9650\n",
      "Epoch 3/20\n",
      "55000/55000 [==============================] - 3s 50us/step - loss: 0.1030 - acc: 0.9716 - val_loss: 0.1227 - val_acc: 0.9686\n",
      "Epoch 4/20\n",
      "55000/55000 [==============================] - 3s 50us/step - loss: 0.0872 - acc: 0.9758 - val_loss: 0.1262 - val_acc: 0.9666\n",
      "Epoch 5/20\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0762 - acc: 0.9782 - val_loss: 0.1306 - val_acc: 0.9702\n",
      "Epoch 6/20\n",
      "55000/55000 [==============================] - 3s 50us/step - loss: 0.0721 - acc: 0.9797 - val_loss: 0.1246 - val_acc: 0.9710\n",
      "Epoch 7/20\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0693 - acc: 0.9803 - val_loss: 0.1293 - val_acc: 0.9696\n",
      "Epoch 8/20\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0666 - acc: 0.9818 - val_loss: 0.1160 - val_acc: 0.9742\n",
      "Epoch 9/20\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0590 - acc: 0.9832 - val_loss: 0.1247 - val_acc: 0.9740\n",
      "Epoch 10/20\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0588 - acc: 0.9831 - val_loss: 0.1316 - val_acc: 0.9720\n",
      "Epoch 11/20\n",
      "55000/55000 [==============================] - 3s 50us/step - loss: 0.0515 - acc: 0.9853 - val_loss: 0.1149 - val_acc: 0.9724\n",
      "Epoch 12/20\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0510 - acc: 0.9855 - val_loss: 0.1206 - val_acc: 0.9730\n",
      "Epoch 13/20\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0491 - acc: 0.9862 - val_loss: 0.1349 - val_acc: 0.9706\n",
      "Epoch 14/20\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0464 - acc: 0.9873 - val_loss: 0.1165 - val_acc: 0.9756\n",
      "Epoch 15/20\n",
      "55000/55000 [==============================] - 3s 52us/step - loss: 0.0447 - acc: 0.9876 - val_loss: 0.1463 - val_acc: 0.9724\n",
      "Epoch 16/20\n",
      "55000/55000 [==============================] - 3s 50us/step - loss: 0.0422 - acc: 0.9885 - val_loss: 0.1196 - val_acc: 0.9728\n",
      "Epoch 17/20\n",
      "55000/55000 [==============================] - 3s 52us/step - loss: 0.0388 - acc: 0.9890 - val_loss: 0.1070 - val_acc: 0.9774\n",
      "Epoch 18/20\n",
      "55000/55000 [==============================] - 3s 48us/step - loss: 0.0381 - acc: 0.9893 - val_loss: 0.1265 - val_acc: 0.9742\n",
      "Epoch 19/20\n",
      "55000/55000 [==============================] - 3s 51us/step - loss: 0.0374 - acc: 0.9899 - val_loss: 0.1474 - val_acc: 0.9686\n",
      "Epoch 20/20\n",
      "55000/55000 [==============================] - 3s 50us/step - loss: 0.0383 - acc: 0.9892 - val_loss: 0.1302 - val_acc: 0.9714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1eaf11414c8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model3.fit(mnist.train.images, mnist.train.labels,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(mnist.validation.images, mnist.validation.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are able to achive great results with only 4% of the original parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T07:09:15.538479Z",
     "start_time": "2019-11-10T07:09:15.096659Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 44us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.13648687947880753, 0.9714]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate(mnist.test.images, mnist.test.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining total files and max len for audio files to be loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T07:09:15.547453Z",
     "start_time": "2019-11-10T07:09:15.539474Z"
    }
   },
   "outputs": [],
   "source": [
    "totalFiles = 1200\n",
    "max_length = 200  #due to a specific file in the test set\n",
    "sr = 16000\n",
    "\n",
    "def getdata(path, str_tr, flag = 0):\n",
    "    wav = []\n",
    "    stft_og = []\n",
    "    stft_aabs = []\n",
    "    z = ['000', '00', '0', '']\n",
    "    \n",
    "    for i in range(totalFiles):\n",
    "        if (i == 0):\n",
    "            j = 0\n",
    "        else:\n",
    "            j = int(math.log10(i))\n",
    "        \n",
    "        s, sr = librosa.load(path + str_tr + z[j] + str(i) + '.wav', sr = None)\n",
    "        if flag ==1:\n",
    "            wav.append(s)   #This is used in SNR Calculations for validation set\n",
    "         \n",
    "\n",
    "        stft = librosa.stft(s, n_fft= 1024, hop_length= 512)\n",
    "        stft_abs = np.abs(stft) \n",
    "        stft_abs = np.pad(stft_abs, ((0,0),(0, max_length-stft.shape[1])), 'constant')\n",
    "        \n",
    "        \n",
    "        stft_og.append(stft)\n",
    "        stft_aabs.append(stft_abs)\n",
    "\n",
    "        \n",
    "    return wav,stft_og, stft_aabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The formula to calculate IBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T07:09:15.554434Z",
     "start_time": "2019-11-10T07:09:15.548450Z"
    }
   },
   "outputs": [],
   "source": [
    "def IBM(S, N):\n",
    "    M = []\n",
    "    for i in range(len(S)):\n",
    "        m_ibm = 1 * (S[i] > N[i])\n",
    "        M.append(m_ibm) \n",
    "    return M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I have saved the train data to a pickle file so I do not need to process it every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T07:10:26.232293Z",
     "start_time": "2019-11-10T07:10:24.829039Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if False:\n",
    "    path = \"timit-homework/tr/\"\n",
    "    trx, X, X_abs = getdata(path, 'trx')  #Train X\n",
    "    trs, S, S_abs = getdata(path, 'trs')  #clean x\n",
    "    trn, N, N_abs = getdata(path, 'trn') #noise  \n",
    "    \n",
    "    M = IBM(S_abs, N_abs) #binary mask\n",
    "    \n",
    "    x_train = np.array(X_abs).swapaxes(1,2)\n",
    "    y_train  = np.array(M).swapaxes(1,2)\n",
    "    \n",
    "    with open('train.pkl', 'wb') as f: \n",
    "        pickle.dump([x_train, y_train], f)\n",
    "else:\n",
    "    with open('train.pkl','rb') as f:  \n",
    "        x_train, y_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a LSTM model with a dropout and Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T07:10:28.398486Z",
     "start_time": "2019-11-10T07:10:27.979608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (None, None, 256)         789504    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, None, 513)         131841    \n",
      "=================================================================\n",
      "Total params: 921,345\n",
      "Trainable params: 921,345\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(CuDNNLSTM(units=256, return_sequences=True\n",
    "               ,kernel_initializer = 'glorot_normal'\n",
    "               , input_shape=(None,513)))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(units=513, activation=\"sigmoid\"))\n",
    "lr = 0.001\n",
    "opt = Adam(lr= lr)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T07:16:41.246928Z",
     "start_time": "2019-11-10T07:10:28.399486Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1200/1200 [==============================] - 7s 6ms/step - loss: 0.4249 - acc: 0.7634\n",
      "Epoch 2/100\n",
      "1200/1200 [==============================] - 4s 4ms/step - loss: 0.3452 - acc: 0.8212\n",
      "Epoch 3/100\n",
      "1200/1200 [==============================] - 5s 4ms/step - loss: 0.3136 - acc: 0.8449\n",
      "Epoch 4/100\n",
      "1200/1200 [==============================] - 4s 4ms/step - loss: 0.2937 - acc: 0.8571\n",
      "Epoch 5/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.2804 - acc: 0.8658\n",
      "Epoch 6/100\n",
      "1200/1200 [==============================] - 5s 4ms/step - loss: 0.2754 - acc: 0.8687\n",
      "Epoch 7/100\n",
      "1200/1200 [==============================] - 5s 4ms/step - loss: 0.2620 - acc: 0.8765\n",
      "Epoch 8/100\n",
      "1200/1200 [==============================] - 4s 4ms/step - loss: 0.2606 - acc: 0.8785\n",
      "Epoch 9/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.2521 - acc: 0.8829\n",
      "Epoch 10/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.2481 - acc: 0.8847\n",
      "Epoch 11/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.2437 - acc: 0.8875\n",
      "Epoch 12/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.2409 - acc: 0.8887\n",
      "Epoch 13/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.2395 - acc: 0.8895A: 0s - loss: 0.2391 - acc\n",
      "Epoch 14/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.2367 - acc: 0.8912\n",
      "Epoch 15/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.2318 - acc: 0.8938A: 0s - loss: 0.2317 - \n",
      "Epoch 16/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.2300 - acc: 0.8949\n",
      "Epoch 17/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.2273 - acc: 0.8962\n",
      "Epoch 18/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.2261 - acc: 0.8970\n",
      "Epoch 19/100\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 0.2238 - acc: 0.8983\n",
      "Epoch 20/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.2204 - acc: 0.9001\n",
      "Epoch 21/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.2192 - acc: 0.9007A: 1s - loss:\n",
      "Epoch 22/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.2172 - acc: 0.9015\n",
      "Epoch 23/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.2160 - acc: 0.9020\n",
      "Epoch 24/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.2133 - acc: 0.9035\n",
      "Epoch 25/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.2117 - acc: 0.9045\n",
      "Epoch 26/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.2097 - acc: 0.9056\n",
      "Epoch 27/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.2096 - acc: 0.9056\n",
      "Epoch 28/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.2074 - acc: 0.9066\n",
      "Epoch 29/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.2051 - acc: 0.9078\n",
      "Epoch 30/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.2054 - acc: 0.9077\n",
      "Epoch 31/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.2027 - acc: 0.9091\n",
      "Epoch 32/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.2017 - acc: 0.9096\n",
      "Epoch 33/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1999 - acc: 0.9105\n",
      "Epoch 34/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.2012 - acc: 0.9098\n",
      "Epoch 35/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1991 - acc: 0.9109\n",
      "Epoch 36/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1974 - acc: 0.9117\n",
      "Epoch 37/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1983 - acc: 0.9115\n",
      "Epoch 38/100\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 0.1972 - acc: 0.9120\n",
      "Epoch 39/100\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 0.1939 - acc: 0.9135\n",
      "Epoch 40/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1940 - acc: 0.9134\n",
      "Epoch 41/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1931 - acc: 0.9139\n",
      "Epoch 42/100\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 0.1910 - acc: 0.9150\n",
      "Epoch 43/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1904 - acc: 0.9152\n",
      "Epoch 44/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1905 - acc: 0.9152\n",
      "Epoch 45/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1897 - acc: 0.9155\n",
      "Epoch 46/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1889 - acc: 0.9160\n",
      "Epoch 47/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1935 - acc: 0.9135\n",
      "Epoch 48/100\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 0.1884 - acc: 0.9162\n",
      "Epoch 49/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1864 - acc: 0.9172A: 1s - l\n",
      "Epoch 50/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1854 - acc: 0.9177\n",
      "Epoch 51/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1871 - acc: 0.9168\n",
      "Epoch 52/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1874 - acc: 0.9166\n",
      "Epoch 53/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1843 - acc: 0.9182\n",
      "Epoch 54/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1837 - acc: 0.9185\n",
      "Epoch 55/100\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 0.1827 - acc: 0.9190\n",
      "Epoch 56/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1822 - acc: 0.9193\n",
      "Epoch 57/100\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 0.1839 - acc: 0.9183\n",
      "Epoch 58/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1814 - acc: 0.9196\n",
      "Epoch 59/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1803 - acc: 0.9201\n",
      "Epoch 60/100\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 0.1803 - acc: 0.9201\n",
      "Epoch 61/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1799 - acc: 0.9203\n",
      "Epoch 62/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1801 - acc: 0.9202\n",
      "Epoch 63/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1795 - acc: 0.9206\n",
      "Epoch 64/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1802 - acc: 0.9202\n",
      "Epoch 65/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1794 - acc: 0.9206\n",
      "Epoch 66/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1777 - acc: 0.9214\n",
      "Epoch 67/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1768 - acc: 0.9218\n",
      "Epoch 68/100\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 0.1771 - acc: 0.9217\n",
      "Epoch 69/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1783 - acc: 0.9211\n",
      "Epoch 70/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1763 - acc: 0.9220\n",
      "Epoch 71/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1760 - acc: 0.9222\n",
      "Epoch 72/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1751 - acc: 0.9226\n",
      "Epoch 73/100\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 0.1747 - acc: 0.9228\n",
      "Epoch 74/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1742 - acc: 0.9231\n",
      "Epoch 75/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1744 - acc: 0.9230A: 0s - loss: 0.1744 -\n",
      "Epoch 76/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1739 - acc: 0.9232\n",
      "Epoch 77/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1735 - acc: 0.9235\n",
      "Epoch 78/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1738 - acc: 0.9233\n",
      "Epoch 79/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1733 - acc: 0.9235\n",
      "Epoch 80/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1730 - acc: 0.9236\n",
      "Epoch 81/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1761 - acc: 0.9222\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1726 - acc: 0.9239\n",
      "Epoch 83/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1713 - acc: 0.9245\n",
      "Epoch 84/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1708 - acc: 0.9248\n",
      "Epoch 85/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1709 - acc: 0.9247\n",
      "Epoch 86/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1705 - acc: 0.9249\n",
      "Epoch 87/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1706 - acc: 0.9249\n",
      "Epoch 88/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1703 - acc: 0.9250\n",
      "Epoch 89/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1701 - acc: 0.9251\n",
      "Epoch 90/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1703 - acc: 0.9250\n",
      "Epoch 91/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1702 - acc: 0.9250\n",
      "Epoch 92/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1710 - acc: 0.9249\n",
      "Epoch 93/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1692 - acc: 0.9255\n",
      "Epoch 94/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1709 - acc: 0.9248\n",
      "Epoch 95/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1752 - acc: 0.9226\n",
      "Epoch 96/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1736 - acc: 0.9235\n",
      "Epoch 97/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1694 - acc: 0.9255\n",
      "Epoch 98/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1678 - acc: 0.9263\n",
      "Epoch 99/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1669 - acc: 0.9267\n",
      "Epoch 100/100\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1667 - acc: 0.9268\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1eafd35d788>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " model.fit(x_train\n",
    "          ,y_train\n",
    "          ,batch_size=10 \n",
    "          ,epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I have saved the validation data to a pickle file so I do not need to process it every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T07:41:26.385328Z",
     "start_time": "2019-11-10T07:41:25.529619Z"
    }
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    path = \"timit-homework/v/\"\n",
    "    vx, VX, VX_abs = getdata(path, 'vx', flag = 1)\n",
    "    vs, VS, VS_abs = getdata(path, 'vs', flag = 1)\n",
    "    with open('validation.pkl', 'wb') as f: \n",
    "        pickle.dump([VX_abs, VX, vs], f)\n",
    "else:\n",
    "    with open('validation.pkl','rb') as f:  \n",
    "        VX_abs, VX, vs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the formula to calculate SNR for the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T07:41:26.391338Z",
     "start_time": "2019-11-10T07:41:26.386325Z"
    }
   },
   "outputs": [],
   "source": [
    "def calcSnr(pred, X, s):\n",
    "    pred = 1 * (pred > 0.5)\n",
    "    pred = pred.T\n",
    "    s_pred = pred * X\n",
    "    s_pred = librosa.istft(s_pred, win_length = 1024, hop_length = 512)\n",
    "    nlen = min(len(s), len(s_pred))\n",
    "    SNR = 10*math.log10((np.sum(s[:nlen]**2))/(np.sum((s[:nlen] - s_pred[:nlen])**2)))\n",
    "    return SNR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SNR is above 10 which is what we were expecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T07:42:00.815375Z",
     "start_time": "2019-11-10T07:41:52.097545Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.76603113979991\n"
     ]
    }
   ],
   "source": [
    "SNR = np.zeros(1200)\n",
    "for i in range(len(VX_abs)):\n",
    "    x_test = VX_abs[i].T[np.newaxis,:,:]\n",
    "    pred = model.predict(x_test)\n",
    "    SNR[i] = calcSnr(pred[0,:VX[i].shape[1]], VX[i], vs[i])\n",
    "print(np.mean(SNR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I have saved the test data to a pickle file so I do not need to process it every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T07:29:07.792237Z",
     "start_time": "2019-11-10T07:29:07.515010Z"
    }
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    path = \"timit-homework/te/\"\n",
    "    totalFiles = 400\n",
    "    tex, TEX, TEX_abs = getdata(path, 'tex', flag = 1)\n",
    "    with open('test.pkl', 'wb') as f: \n",
    "        pickle.dump([tex, TEX,  TEX_abs], f)\n",
    "else:\n",
    "    with open('test.pkl','rb') as f:  \n",
    "        tex, TEX,  TEX_abs= pickle.load(f)                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising test data and exporting to a final file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T07:42:25.052822Z",
     "start_time": "2019-11-10T07:42:25.047835Z"
    }
   },
   "outputs": [],
   "source": [
    "def testfinal(pred, X, i):\n",
    "    pred = 1 * (pred > 0.5)\n",
    "    pred = pred.T\n",
    "    s_pred = pred * X\n",
    "    s_pred = librosa.istft(s_pred, win_length = 1024, hop_length = 512)\n",
    "    librosa.output.write_wav('timit-homework/results/final' + str(i) + '.wav', s_pred, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T07:42:29.777498Z",
     "start_time": "2019-11-10T07:42:26.301691Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(TEX_abs)):\n",
    "    x_test = TEX_abs[i].T[np.newaxis,:,:]\n",
    "    pred = model.predict(x_test)\n",
    "    testfinal(pred[0,:TEX[i].shape[1],:], TEX[i], i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras] *",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
